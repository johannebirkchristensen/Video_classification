{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Add the directory to sys.path\n",
    "os.chdir(\"/zhome/b6/d/154958/Video_detection/Video_classification\")\n",
    "datasets_dir = '/dtu/blackhole/16/155094/Video_classification'\n",
    "sys.path.append(datasets_dir)\n",
    "from datasets1 import FrameImageDataset, FrameVideoDataset\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "    root_dir='/dtu/blackhole/16/155094/ufc101',\n",
    "    split='train', \n",
    "    transform=None\n",
    "):\n",
    "        self.frame_paths = sorted(glob(f'{root_dir}/frames/{split}/*/*/*.jpg'))\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.df = pd.read_csv(f'{root_dir}/metadata/{split}.csv')\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.frame_paths)\n",
    "\n",
    "    def _get_meta(self, attr, value):\n",
    "        return self.df.loc[self.df[attr] == value]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_path = self.frame_paths[idx]\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        video_name = frame_path.split('/')[-2]\n",
    "        video_meta = self._get_meta('video_name', video_name)\n",
    "        label = video_meta['label'].item()\n",
    "        \n",
    "        the_split = frame_path.split(\"/\")\n",
    "        \n",
    "        first = the_split[-3]\n",
    "        second = the_split[-2]\n",
    "        \n",
    "        flow_paths = sorted(glob(f'{self.root_dir}/flows/{self.split}/{first}/{second}/*.npy'))\n",
    "        frame = Image.open(frame_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "        else:\n",
    "            frame = T.ToTensor()(frame)\n",
    "\n",
    "        flow_tensor = np.array([np.load(x) for x in flow_paths])\n",
    "        flow_tensor = torch.from_numpy(flow_tensor)\n",
    "        return frame, label, flow_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Dataset directories\n",
    "root_dir = '/dtu/datasets1/02516/ucf101_noleakage'\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = FrameImageDataset(root_dir=root_dir, split='train', transform=None)\n",
    "val_dataset = FrameImageDataset(root_dir=root_dir, split='val', transform=None)\n",
    "test_dataset = FrameImageDataset(root_dir=root_dir, split='test', transform=None)\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwoStreamConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(TwoStreamConvNet, self).__init__()\n",
    "\n",
    "        # Spatial stream (RGB images)\n",
    "        self.spatial_stream = nn.Sequential(\n",
    "            nn.Conv2d(3, 36, kernel_size=7, stride=2, padding=3),  # conv1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5),  # norm\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool\n",
    "\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Conv2d(36, 96, kernel_size=5, stride=2, padding=2),  # conv2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5),  # norm\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool\n",
    "\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Conv2d(96, 128, kernel_size=3, stride=1, padding=1),  # conv5\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # pool\n",
    "        )\n",
    "\n",
    "        # Temporal stream (stacked optical flow)\n",
    "        self.temporal_stream = nn.Sequential(\n",
    "            nn.Conv2d(18, 36, kernel_size=7, stride=2, padding=3),  # conv1\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5),  # norm\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool\n",
    "            nn.Dropout(0.7),\n",
    "\n",
    "            nn.Conv2d(36, 96, kernel_size=5, stride=2, padding=2),  # conv2\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5),  # norm\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # pool\n",
    "            \n",
    "            nn.Dropout(0.7),\n",
    "            nn.Conv2d(96, 128, kernel_size=3, stride=1, padding=1),  # conv3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # pool\n",
    "\n",
    "        )\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 7 * 7 * 2, 256),  # Concatenated features from both streams\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(128, num_classes)  # Output probabilities for `num_classes`\n",
    "        )\n",
    "\n",
    "    def forward(self, spatial_input, temporal_input):\n",
    "        # Forward pass through both streams\n",
    "        spatial_features = self.spatial_stream(spatial_input)\n",
    "        temporal_features = self.temporal_stream(temporal_input)\n",
    "\n",
    "        # Flatten features\n",
    "        spatial_features = spatial_features.view(spatial_features.size(0), -1)\n",
    "        temporal_features = temporal_features.view(temporal_features.size(0), -1)\n",
    "\n",
    "        # Concatenate features from both streams\n",
    "        combined_features = torch.cat((spatial_features, temporal_features), dim=1)\n",
    "\n",
    "        # Classification\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Instantiate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "class FusionStreamModel_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_channels= 3, dropOutVal = 0.5):\n",
    "        super(FusionStreamModel_CNN, self).__init__()\n",
    "        \n",
    "        \n",
    "    \n",
    "       # Convolutional Feature Extraction\n",
    "        self.backbone_Temporal = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "\n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            \n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Sequential(\n",
    "            nn.Linear(12544, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes) \n",
    "        ))\n",
    "\n",
    "        \n",
    "        # Convolutional Feature Extraction\n",
    "        self.backbone_Spatial = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "\n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            \n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            \n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout2d(dropOutVal),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  \n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Sequential(\n",
    "            nn.Linear(12544, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes) \n",
    "        ))\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, frame,flow):\n",
    "        # Extract features using EfficientNet\n",
    "        \n",
    "        spatial = self.backbone_Spatial(frame)\n",
    "        spatial = torch.softmax(spatial, dim= 1)\n",
    "        \n",
    "        temporal = self.backbone_Temporal(flow)\n",
    "        temporal = torch.softmax(temporal, dim= 1)\n",
    "\n",
    "        output = torch.stack((spatial,temporal)).mean(dim=0)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "Train Loss: 1.9978, Train Accuracy: 28.54%\n",
      "Val Loss: 1.8303, Val Accuracy: 41.50%\n",
      "Epoch [2/5]\n",
      "Train Loss: 1.3012, Train Accuracy: 53.90%\n",
      "Val Loss: 1.5194, Val Accuracy: 50.92%\n",
      "Epoch [3/5]\n",
      "Train Loss: 0.8581, Train Accuracy: 71.00%\n",
      "Val Loss: 1.4661, Val Accuracy: 46.92%\n",
      "Epoch [4/5]\n",
      "Train Loss: 0.5680, Train Accuracy: 80.40%\n",
      "Val Loss: 1.6221, Val Accuracy: 48.83%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     22\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame, labels, flow_tensor \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     25\u001b[0m     frame, labels, flow_tensor \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device), flow_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/blackhole/0e/154958/miniconda/envs/IDLICV_2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/dtu/blackhole/0e/154958/miniconda/envs/IDLICV_2/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/dtu/blackhole/0e/154958/miniconda/envs/IDLICV_2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/dtu/blackhole/0e/154958/miniconda/envs/IDLICV_2/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m, in \u001b[0;36mFrameImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     frame \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mToTensor()(frame)\n\u001b[0;32m---> 43\u001b[0m flow_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mload(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flow_paths])\n\u001b[1;32m     44\u001b[0m flow_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(flow_tensor)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frame, label, flow_tensor\n",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     frame \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mToTensor()(frame)\n\u001b[0;32m---> 43\u001b[0m flow_tensor \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flow_paths])\n\u001b[1;32m     44\u001b[0m flow_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(flow_tensor)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frame, label, flow_tensor\n",
      "File \u001b[0;32m/dtu/blackhole/0e/154958/miniconda/envs/IDLICV_2/lib/python3.9/site-packages/numpy/lib/npyio.py:434\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    432\u001b[0m _ZIP_SUFFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x05\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# empty zip files start with this\u001b[39;00m\n\u001b[1;32m    433\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX)\n\u001b[0;32m--> 434\u001b[0m magic \u001b[38;5;241m=\u001b[39m \u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# model = FusionStreamModel_CNN(input_channels=18)\n",
    "# Loss and Optimizer\n",
    "model = TwoStreamConvNet(num_classes=10)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Metrics storage\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "# Training and Validation Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for frame, labels, flow_tensor in train_loader:\n",
    "        frame, labels, flow_tensor = frame.to(device), labels.to(device), flow_tensor.to(device)\n",
    "        # Forward pass\n",
    "        shape_0 = flow_tensor.shape[0]\n",
    "        shape_1 = flow_tensor.shape[1]\n",
    "        shape_2 = flow_tensor.shape[2]\n",
    "        shape_3 = flow_tensor.shape[3]\n",
    "        shape_4 = flow_tensor.shape[4]\n",
    "        \n",
    "\n",
    "        flow_tensor = flow_tensor.view(shape_0,shape_1*shape_2, shape_3,shape_4)\n",
    "        \n",
    "        outputs = model(frame,flow_tensor)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for frame, labels, flow_tensor in val_loader:\n",
    "            \n",
    "            frame, labels, flow_tensor = frame.to(device), labels.to(device), flow_tensor.to(device)\n",
    "            # Forward pass\n",
    "            shape_0 = flow_tensor.shape[0]\n",
    "            shape_1 = flow_tensor.shape[1]\n",
    "            shape_2 = flow_tensor.shape[2]\n",
    "            shape_3 = flow_tensor.shape[3]\n",
    "            shape_4 = flow_tensor.shape[4]\n",
    "            \n",
    "\n",
    "            flow_tensor = flow_tensor.view(shape_0,shape_1*shape_2, shape_3,shape_4)\n",
    "\n",
    "            outputs = model(frame, flow_tensor)\n",
    "            # print(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100 * correct / total\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDLICV_2",
   "language": "python",
   "name": "idlicv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
