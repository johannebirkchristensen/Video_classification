{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/zhome/45/0/155089/Deeplearning_in_computer_vision/Video_classification/')\n",
    "\n",
    "from datasets import FrameImageDataset, FrameVideoDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definér root_dir og transformation\n",
    "root_dir = '/dtu/blackhole/16/155094/ufc101'\n",
    "#transform = T.Compose([T.Resize((64, 64)),T.ToTensor()])\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = FrameVideoDataset(root_dir=root_dir, split='train', transform=transform, stack_frames = True)\n",
    "val_dataset = FrameVideoDataset(root_dir=root_dir, split='val', transform=transform, stack_frames = True)\n",
    "test_dataset = FrameVideoDataset(root_dir=root_dir, split='test', transform=transform, stack_frames = True)\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# frames in frameimage_loader is torch.Size([8, 3, 64, 64])\n",
    "#frameimage_loader = DataLoader(frameimage_dataset,  batch_size=8, shuffle=False)\n",
    "# frames in frameimage_loader is torch.Size([8, 3, 10, 64, 64])\n",
    "#framevideostack_loader = DataLoader(framevideostack_dataset, batch_size=16, shuffle=True)\n",
    "# 10 frames pr video, each with Video x: torch.Size([8, 3, 64, 64])\n",
    "#framevideolist_loader = DataLoader(framevideolist_dataset,  batch_size=8, shuffle=False)\n",
    "\n",
    "# train on the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antal billeder i træningssættet: 500\n",
      "Antal billeder i valssættet: 120\n",
      "Antal billeder i testsættet: 120\n"
     ]
    }
   ],
   "source": [
    "print(f\"Antal billeder i træningssættet: {len(train_dataset)}\")\n",
    "print(f\"Antal billeder i valssættet: {len(val_dataset)}\")\n",
    "print(f\"Antal billeder i testsættet: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot billeder og labels - lad være med at kør mig:)\n",
    "for frames, labels in frameimage_loader:\n",
    "    fig, axes = plt.subplots(1, len(frames), figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        # Konverter tensor til numpy-array og transponér til (H, W, C)\n",
    "        img = frames[i].permute(1, 2, 0).numpy()\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Label: {labels[i].item()}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "    #break  # Kun vis ét batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spatial convolutions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Første konvolutionslag\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # [batch_size, 3, 64, 64] -> [batch_size, 16, 64, 64]\n",
    "        \n",
    "        # Andet konvolutionslag\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # [batch_size, 16, 64, 64] -> [batch_size, 32, 64, 64]\n",
    "        \n",
    "        # Tredje konvolutionslag\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # [batch_size, 32, 64, 64] -> [batch_size, 64, 64, 64]\n",
    "        \n",
    "        # Global Average Pooling, som reducerer højden og bredden til 1\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # [batch_size, 64, 64, 64] -> [batch_size, 64, 1, 1]\n",
    "        \n",
    "        # Fuldt forbundet lag til feature reduktion\n",
    "        self.fc = nn.Linear(64, 128)  # [batch_size, 64] -> [batch_size, 128]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passerer gennem konvolutionslagene med ReLU aktivering\n",
    "        x = F.relu(self.conv1(x))  # Output: [batch_size, 16, 64, 64]\n",
    "        x = F.relu(self.conv2(x))  # Output: [batch_size, 32, 64, 64]\n",
    "        x = F.relu(self.conv3(x))  # Output: [batch_size, 64, 64, 64]\n",
    "        \n",
    "        # Global Average Pooling (reducerer spatial dimensioner til 1x1)\n",
    "        x = self.global_avg_pool(x)  # Output: [batch_size, 64, 1, 1]\n",
    "        \n",
    "        # Fladgør tensoren til at passe til det fuldt forbundne lag\n",
    "        x = x.view(x.size(0), -1)  # Output: [batch_size, 64] -> Flad til [batch_size, 64]\n",
    "        \n",
    "        # Fuldt forbundet lag for at få 128 features\n",
    "        x = self.fc(x)  # Output: [batch_size, 128]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The late fusion model, where we use the spatial convolutions model above ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 128)  # Output 128-dimensional features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.global_avg_pool(x)  # [batch_size, 64, 1, 1]\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)  # [batch_size, 128]\n",
    "        return x\n",
    "\n",
    "\n",
    "class LateFusionModel(nn.Module):\n",
    "    def __init__(self, base_model, feature_dim, num_classes):\n",
    "        super(LateFusionModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, video_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_tensor: Tensor of shape [batch_size, C, T, H, W]\n",
    "        Returns:\n",
    "            output: Tensor of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        batch_size, C, T, H, W = video_tensor.shape\n",
    "        \n",
    "        # Reshape to process all frames as a batch\n",
    "        frames = video_tensor.permute(0, 2, 1, 3, 4)  # [batch_size, T, C, H, W]\n",
    "        frames = frames.reshape(-1, C, H, W)  # [batch_size * T, C, H, W]\n",
    "        \n",
    "        # Extract features for all frames in parallel\n",
    "        frame_features = self.base_model(frames)  # [batch_size * T, feature_dim]\n",
    "        frame_features = frame_features.view(batch_size, T, -1)  # [batch_size, T, feature_dim]\n",
    "        \n",
    "        # Late Fusion: aggregate features across the time dimension\n",
    "        video_features = frame_features.mean(dim=1)  # [batch_size, feature_dim]\n",
    "\n",
    "        # Classification\n",
    "        logits = self.fc(video_features)  # [batch_size, num_classes]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=50):\n",
    "    # Gem trænings- og valideringstab og nøjagtighed\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []  # Korrekt initialisering af listerne\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Træningstrin\n",
    "        model.train()  # Sæt modellen til træningstilstand\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Nulstil gradienterne\n",
    "\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "\n",
    "            loss = criterion(outputs, labels)  # Beregn loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Opdater vægtene\n",
    "\n",
    "            # Beregn nøjagtighed\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Beregn gennemsnitligt træningstab og nøjagtighed\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = (correct_predictions / total_predictions) * 100\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Valideringstrin\n",
    "        model.eval()  # Sæt modellen til evalueringsmodus\n",
    "        val_loss = 0.0\n",
    "        val_correct_predictions = 0\n",
    "        val_total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():  # Ingen gradientberegning under validering\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)  # Forward pass\n",
    "\n",
    "                loss = criterion(outputs, labels)  # Beregn loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Beregn valideringsnøjagtighed\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct_predictions += (predicted == labels).sum().item()\n",
    "                val_total_predictions += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = (val_correct_predictions / val_total_predictions) * 100\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # Print status\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()  # Sæt modellen til evalueringsmodus\n",
    "    test_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Ingen gradientberegning under test\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)  # Forward pass\n",
    "\n",
    "            loss = criterion(outputs, labels)  # Beregn loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Beregn testnøjagtighed\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return avg_test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Training Loss: 2.3055, Training Accuracy: 9.40%\n",
      "Validation Loss: 2.2936, Validation Accuracy: 14.17%\n",
      "Epoch 2/50\n",
      "Training Loss: 2.2991, Training Accuracy: 10.40%\n",
      "Validation Loss: 2.2821, Validation Accuracy: 15.83%\n",
      "Epoch 3/50\n",
      "Training Loss: 2.2744, Training Accuracy: 15.40%\n",
      "Validation Loss: 2.2439, Validation Accuracy: 19.17%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Base model\n",
    "base_model = SimpleCNN().to(device)  # SimpleCNN modellen\n",
    "\n",
    "feature_dim = 128  # Output dimension af SimpleCNN's sidste fuldt forbundne lag\n",
    "num_classes = 10  # For UFC101 dataset\n",
    "\n",
    "# Initialiser model, criterion og optimizer\n",
    "model = LateFusionModel(base_model, feature_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # For eksempel\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Træn og valider modellen\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = train_and_validate(\n",
    "    model, train_loader, val_loader, criterion, optimizer, device, num_epochs=50\n",
    ")\n",
    "\n",
    "# Evaluér modellen på testdatasættet efter træning\n",
    "test_loss, test_accuracy = test(model, test_loader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
